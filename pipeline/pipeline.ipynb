{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from langchain_text_splitters import (\n",
    "    TextSplitter,\n",
    "    # RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "from ranx import Qrels, Run, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "###########\n",
    "# SOURCES #\n",
    "###########\n",
    "class Source(BaseModel):\n",
    "    id: str\n",
    "    url: str\n",
    "    name: str\n",
    "    desc: str\n",
    "    type: Literal[\"moodle\", \"file\", \"web\", \"tg\"] = Field(\"file\")\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.id.__hash__()\n",
    "\n",
    "\n",
    "class MoodleSource(Source):\n",
    "    course_id: str\n",
    "    course_url: str\n",
    "    course_name: str\n",
    "    type: Literal[\"moodle\"] = Field(\"moodle\")\n",
    "\n",
    "\n",
    "class FileSource(Source):\n",
    "    type: Literal[\"file\"] = Field(\"file\")\n",
    "\n",
    "\n",
    "class WebSource(Source):\n",
    "    type: Literal[\"web\"] = Field(\"web\")\n",
    "\n",
    "\n",
    "class TelegramSource(Source):\n",
    "    type: Literal[\"tg\"] = Field(\"tg\")\n",
    "\n",
    "\n",
    "#########\n",
    "# UTILS #\n",
    "#########\n",
    "class Chunk(BaseModel):\n",
    "    index: int = Field(ge=0)\n",
    "    source_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "##########\n",
    "# SEARCH #\n",
    "##########\n",
    "class SearchQuery(BaseModel):\n",
    "    text: str\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    source: Source\n",
    "    distance: float\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.source.id.__hash__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# meta datas\n",
    "DATA_PATH = Path(\"../data\")\n",
    "META_FILE_PATH = DATA_PATH / \"meta.json\"\n",
    "\n",
    "# text data\n",
    "TEXTS_PATH = Path(\"../texts\")\n",
    "PREPROCESSED_PATH = Path(\"../preprocessed\")\n",
    "\n",
    "# all related to validation\n",
    "VALIDATION_PATH = Path(\"../validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources_info(meta_file_path: Path) -> dict[str, Source]:\n",
    "    with open(meta_file_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "        meta_data = json.load(meta_file)\n",
    "\n",
    "    sources_info: dict[str, Source] = {}\n",
    "    for data in meta_data:\n",
    "        source: Source = Source.model_validate_json(json.dumps(data), strict=True)\n",
    "        sources_info[source.id] = source\n",
    "\n",
    "    return sources_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_info(meta_file_path: Path, texts_path: Path, text_splitter: TextSplitter) -> dict[int, Chunk]:\n",
    "    # log missing files\n",
    "    logging.basicConfig(\n",
    "        filename=\"missing.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    with open(meta_file_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "        meta_data = json.load(meta_file)\n",
    "\n",
    "    # get current available sources list\n",
    "    sources: list[Source] = []\n",
    "    for data in meta_data:\n",
    "        source: Source = Source.model_validate_json(json.dumps(data), strict=True)\n",
    "        sources.append(source)\n",
    "\n",
    "    index = 0\n",
    "    chunks_info: dict[int, Chunk] = {}\n",
    "    for source in tqdm(sources, total=len(sources), desc=\"Split sources on chunks\", unit=\"source\"):\n",
    "        source_text_path = texts_path / (source.name + \".txt\")\n",
    "\n",
    "        # save not found files into logs\n",
    "        if not os.path.exists(source_text_path):\n",
    "            logging.info(source.id)\n",
    "            continue\n",
    "\n",
    "        # otherwise get their content\n",
    "        with open(source_text_path, \"r\", encoding=\"utf-8\") as text_file:\n",
    "            text = text_file.read()\n",
    "\n",
    "        # update info dict with current source's chunks\n",
    "        for chunk_text in text_splitter.split_text(text):\n",
    "            chunk = Chunk(index=index, source_id=source.id, text=chunk_text)\n",
    "            chunks_info[index] = chunk\n",
    "            index += 1\n",
    "\n",
    "    return chunks_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_by_chunk(chunk_index: int, chunks_info: dict[int, Chunk], sources_info: dict[str, Source]) -> Source:\n",
    "    if not chunks_info.get(chunk_index):\n",
    "        raise ValueError(f\"Chunk {chunk_index} not found\")\n",
    "\n",
    "    chunk: Chunk = chunks_info[chunk_index]\n",
    "\n",
    "    if not sources_info.get(chunk.source_id):\n",
    "        raise ValueError(f\"Source {chunk.source_id} not found\")\n",
    "\n",
    "    return sources_info[chunk.source_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    texts: list[str],\n",
    "    model: SentenceTransformer,\n",
    "    show_progress_bar: bool = False,\n",
    ") -> np.ndarray:\n",
    "    embeddings: np.ndarray = model.encode(texts, batch_size=64, show_progress_bar=show_progress_bar)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    query_embedding: np.ndarray,\n",
    "    embeddings: np.ndarray,\n",
    "    sources_info: dict[str, Source],\n",
    "    chunks_info: dict[int, Chunk],\n",
    "    strategy: Literal[\"base\", \"majority_vote\"] = \"base\",\n",
    "    threshold: float = 0.4,\n",
    "    k: int = 10,\n",
    ") -> list[SearchResult]:\n",
    "    \"\"\"Outputs results of semantic search with reranking strategy used among given sources.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray): vector representation of the query of size (1, {embedding_size}).\n",
    "        embeddings (np.ndarray): vector representations of a corpus.\n",
    "        chunks (list[Chunk]): little pieces of sources.\n",
    "        strategy (Literal[\"base\", \"majority_vote\"], optional): reranking strategy among found sources. Defaults to \"base\".\n",
    "        threshold (float, optional): min value of similarity to be present on candidate. Defaults to 0.4.\n",
    "        k (int, optional): final maximum number of sources. Defaults to 10.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: signal that mentioned reranking strategy does not exist.\n",
    "\n",
    "    Returns:\n",
    "        list[SearchResult]: final output of sources.\n",
    "    \"\"\"\n",
    "    results = util.semantic_search(query_embedding, embeddings, top_k=50, score_function=util.dot_score)\n",
    "    assert len(results) == 1\n",
    "\n",
    "    search_results: list[SearchResult] = []\n",
    "    for result in results[0]:\n",
    "        chunk_index = result[\"corpus_id\"]\n",
    "        source: Source = get_source_by_chunk(chunk_index, chunks_info, sources_info)\n",
    "        search_result = SearchResult(text=\"\", source=source, distance=result[\"score\"])\n",
    "        search_results.append(search_result)\n",
    "\n",
    "    # by distance\n",
    "    new_results: list[SearchResult] = []\n",
    "    if strategy == \"base\":\n",
    "        added_source_ids = set()\n",
    "        for search_result in search_results:\n",
    "            if search_result.distance < threshold:  # skip if lower than threshold\n",
    "                continue\n",
    "\n",
    "            if search_result.source.id not in added_source_ids:\n",
    "                added_source_ids.add(search_result.source.id)\n",
    "                new_results.append(search_result)\n",
    "\n",
    "    # count appearance of chunk's belonging to a source\n",
    "    elif strategy == \"majority_vote\":\n",
    "        # apply majority vote\n",
    "        counter = Counter(\n",
    "            [search_result.source for search_result in search_results if search_result.distance > threshold]\n",
    "        )\n",
    "        most_common = counter.most_common(10)\n",
    "\n",
    "        # filter and leave unique documents (a bit of crutch O(n^2))\n",
    "        for source, _ in most_common:\n",
    "            for result in search_results:\n",
    "                if source.id == result.source.id:\n",
    "                    new_results.append(result)\n",
    "                    break\n",
    "    else:\n",
    "        raise ValueError(\"Strategy is not supported\")\n",
    "\n",
    "    return new_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_file(texts_path: Path, source_name: str):\n",
    "    source_path = texts_path / (source_name + \".txt\")\n",
    "    if not os.path.exists(source_path):\n",
    "        print(f\"File {source_path} not found\")\n",
    "        return\n",
    "\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "sources_info = load_sources_info(META_FILE_PATH)  # type: ignore\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# model\n",
    "# MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'  # SOTA\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import TextSplitter, RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# separators = RecursiveCharacterTextSplitter.get_separators_for_language(Language.MARKDOWN) + \\\n",
    "#              RecursiveCharacterTextSplitter.get_separators_for_language(Language.HTML)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=5000,\n",
    "#     chunk_overlap=500,\n",
    "#     len\n",
    "#     add_start_index=True,\n",
    "#     separators=separators\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Split sources on chunks: 100%|██████████| 949/949 [01:42<00:00,  9.26source/s]\n"
     ]
    }
   ],
   "source": [
    "# split on chunks\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=25, model_name=MODEL_NAME)\n",
    "chunks_info = load_chunks_info(META_FILE_PATH, TEXTS_PATH, text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01fea6d9f041f3ae069a2144ed1ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PodYapolsky\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\search-experiments-AGuved54-py3.11\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42075, 384)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed text chunks\n",
    "texts = [chunk.text for chunk in chunks_info.values()]\n",
    "embeddings = embed(texts, model, True)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 2. 18. проживающие обязаны поддерживать атмосферу доброжелательности, сотрудничества и взаимного уважения, соблюдать общепринятые морально - этические нормы, не допускать конфликтных ситуации, не употреблять ненормативную лексику, не высказываться грубо по отношению к персоналу комплекса и униве\n",
      "techniques for translating categorical propositions into standard form, but for now we may restrict our attention to those that are already in standard form. the words ‘ ‘ all, ’ ’ ‘ ‘ no, ’ ’ and ‘ ‘ some ’ ’ are called < b > quantifiers < / b > because they specify how much of the subject class is included in or excluded from the predicate class. the first form above asserts that the whole subject class is included in the predicate class, the second that the whole subject class is excluded from the predicate class, and so on. ( incidentally, in formal deductive logic the word ‘ ‘ some ’ ’ always means at least one. ) the letters ‘ ‘ < i > s < / i > ’ ’ and ‘ ‘ < i > p < / i > ’ ’ stand respectively for the subject and predicate terms, and the words ‘ ‘ are ’ ’ and ‘ ‘ are not ’ ’ are called the because they link ( or ‘ ‘ couple ’ ’ ) the subject term with the predicate term. < b > copula < / b > consider the following example : all members of the american medical association are persons holding degrees from recognized academic institutions. this standard - form categorical proposition is analyzed as\n",
      "it is necessary to convert a sentence < i > a < / i > into a sentence < i > a < / i > < i > ′ < / i > in clause form, such that < i > a < / i > is unsatisfiable iff < i > a < / i > < i > ′ < / i > is unsatisfiable. the conversion process is defined below. < b > 8. 2 formulae in clause form < / b > first, we define the notion of a formula in clause form. < b > definition 8. 2. 1 < / b > as in the propositional case, a < i > literal < / i > is either an atomic formula < i > b < / i >, or the negation < i > b < / i > of an atomic formula. given a literal < i > l < / i >, its < i > ¬ < / i > < i > conjugate < / i > < i > l < / i > is defined such that, if < i > l < / i > = < i > b < / i > then < i > l < / i > = < i > b < / i >, else\n",
      "> < i > | | < / i > 1 < i >. < / i > < i > [UNK] | | < / i > < i > x < / i > < i > | | < / i > 2 equivalent < i > [UNK] | | < / i > < i > x < / i > < i > | | < / i > 1. two dimensional manifold regions and boundaries a limit of a function of several variables continuous functions partial derivativ - - - - - functions of several variables < h1 > the definition of the distance < / h1 > for distance between two points < i > x < / i > and < i > y < / i > we will use the form : < i > ρ < / i > ( < i > x < / i > < i >, < / i > < i > y < / i > ) < i > x < / i > < i > y < / i > < i > ≡ | | < / i > < i > − < / i > < i > | | < / i > without specification the kind of the norm. mostly due to their equivalence of the norms we will assume the euclidean norm : < i > n < / i >\n",
      "planes. normal vectors are perpendicular, planes still intersect! ( b ) need three orthogonal'yectors to span the whole orthogonal complement in r5. ( c ) lines can meet without being orthogonal. = = 51. when ab 0, the column sace of b is contained in the nullspace of a. therefore the dimension of c ( b ) < dimension of n ( a ). this means rank ( b ) < 4 - rank ( a ). probleln set 3. 2, page 151. > > 1. ( a ) ( x + y ) / 2 fy ( arithmetic mean geometric mean of x and y ). < < ( b ) ilx + y l12 ( 1ix ll + lly i1 ) 2 meansthat ( x + y ) t ( x + y ) iix i12 + 21ix ll lly ll + lly i12. the left - hand side is x t x + 2x t y + y t y. after cancelling this is x t y < ilx ii ii y ii. 3. p = = ( 10 / 3, 10 / 3, 10 / 3 ) ; ( 5 / 9, 10 / 9, 10 / 9 )., 1. 5\n",
      "/ b > < i > a < / i > < b > ) < / b >. 4. < i > ” linear algebra and applications ”, pdf pages 96 – 106 < / i > what does partial and full solutions means < a href ='https : / / www. youtube. com / watch? v = ggwykes - n6e'> 5. the big picture of linear algebra < / a > < i > extra for now < / i > if you want to get the global view of four subspaces 6. understand the application from next few slides and make hw tasks! oleg bulichev agla2 1 - - - - - < b > null space : application from robotics < / b > < i > video < / i > oleg bulichev agla2 2 - - - - - < b > null space : application from robotics < / b > < i > theory ( 1 ) < / i > let us consider differential kinematic relationship : [UNK] < b > < i > x < / i > < / b > < b > = < / b > < b > j < / b > < b > ( < / b > < b > q < / b >\n",
      "b > < i > m < / i > < / b > < b > < i > 1 < / i > < / b > < b > < i > 2 < / i > < / b > < b > < i > 1 < / i > < / b > < b > < i > 2 < / i > < / b > then a multicast protocol that implements causal ordering will obey fifo ordering since < b > < i > m < / i > < / b > < b > < i > m < / i > < / b > < b > < i > 1 < / i > < / b > < b > < i > 2 < / i > < / b > < b > reverse is not true! fifo ordering does not imply < / b > < b > causal ordering. < / b > distributed & network programming 32 - - - - - < h2 > causal ordering : implementation < / h2 > < b > key idea < / b > each process uses a < b > precedence vector < / b > which is similar to vector clock we saw before : • difference : vector ’ s values updated upon send and receive events only • vector includes sequence numbers of each process < b\n",
      "mechanical < b > a. 3. 3. 284. 8 parking structure. < / b > control push - button - type elevators to transfer vehicles from one floor to another. motor vehicles are permitted to be parked by the driver or an attendant or are permitted to be parked mechanically by automated facilities. where automated - type parking is provided, the operator of those facilities is permitted either to remain at the entry level or to travel to another level. motor fuel is permitted to be dispensed, and motor vehicles are permitted to be serviced in a parking structure in accordance with nfpa 30a. [ < b > 88a, < / b > 2019 ] in determining openings in exterior walls, doors or access panels are permitted to < b > a. 3. 3. 284. 12 underground structure. < / b > be included. windows are also permitted to be included, provided that they are openable or provide a breakable glazed area. floor levels that are located not more than 30 ft ( 9. 1 m ) below the lowest level with an exit discharge can be considered a basement. see 3. 3. 33. < b > a. 3. 3. 291 tent. < / b > a tent might also include a temporary tensioned - membrane\n",
      "< i > < подпись > < / i > < i > < расшифровка подписи > < / i > < i > < дата > < / i > < i > проректор < / i > < i > < подпись > < / i > < i > < расшифровка подписи > < / i > - - - - - 28 < i > < дата > < / i > < b > конец формы < / b > - - - - - 29 приложение № 6 к положению о порядке и основаниях перевода, отчисления и восстановления обучающихся и предоставления академическ\n"
     ]
    }
   ],
   "source": [
    "for text in texts[50::5000]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(source=Source(id='module-108403.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=108403', name='module-108403.pdf', desc='Lecture Week 2 Part I (Network Characteristics)', type='moodle'), distance=0.4472094476222992),\n",
       " SearchResult(source=Source(id='module-82752.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=82752', name='module-82752.pdf', desc='Lecture 1 Slides', type='moodle'), distance=0.44532811641693115),\n",
       " SearchResult(source=Source(id='module-83884.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=83884', name='module-83884.pdf', desc='Tutorial 7 Slides', type='moodle'), distance=0.4386726915836334),\n",
       " SearchResult(source=Source(id='module-92978.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92978', name='module-92978.pdf', desc='Lecture 14. Flow networks. Ford-Fulkerson. Max flow min cut', type='moodle'), distance=0.43859079480171204),\n",
       " SearchResult(source=Source(id='https://innopolis.university/upload/iblock/0b3/92tmskeuzgrgqcs5r64xlb1g8g9i4tf7/Отчет_о_результатах_самообследования__2024__на_сайт.pdf', url='https://innopolis.university/upload/iblock/0b3/92tmskeuzgrgqcs5r64xlb1g8g9i4tf7/Отчет_о_результатах_самообследования__2024__на_сайт.pdf', name='Отчет_о_результатах_самообследования__2024__на_сайт.pdf', desc='Отчет о результатах самообследования Автономной некоммерческой организацией высшего образования «Университет Иннополис» за 2023 год', type='file'), distance=0.43394529819488525),\n",
       " SearchResult(source=Source(id='module-79154.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=79154', name='module-79154.pdf', desc='Graph Theory (main book)', type='moodle'), distance=0.4296361207962036),\n",
       " SearchResult(source=Source(id='module-89468.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=89468', name='module-89468.pdf', desc='Introduction to Linear Algebra', type='moodle'), distance=0.42538952827453613),\n",
       " SearchResult(source=Source(id='module-78518.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=78518', name='module-78518.pdf', desc='Lab 4 (bul)', type='moodle'), distance=0.4132364094257355),\n",
       " SearchResult(source=Source(id='module-109578.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=109578', name='module-109578.pdf', desc='Lecture Week 10 Part II (Forwarding and Routing. Router Architecture)', type='moodle'), distance=0.4123547673225403),\n",
       " SearchResult(source=Source(id='module-89469.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=89469', name='module-89469.pdf', desc='Numerical Linear Algebra', type='moodle'), distance=0.401311457157135)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"Burmykov Networks course lecture 11\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding, embeddings, sources_info, chunks_info, strategy=\"base\", threshold=0.4, k=10\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(source=Source(id='module-92034.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92034', name='module-92034.pdf', desc='Tutorial 08 - SOLID', type='moodle'), distance=0.46830806136131287),\n",
       " SearchResult(source=Source(id='module-92216.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92216', name='module-92216.pdf', desc='2023 SSAD 14 Command, Chain or Resp, SOLID', type='moodle'), distance=0.4437078535556793),\n",
       " SearchResult(source=Source(id='module-89466.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=89466', name='module-89466.pdf', desc='Linear Algebra and Its Applications', type='moodle'), distance=0.4280555248260498)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"SOLID principles examples\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding,\n",
    "    embeddings,\n",
    "    sources_info,\n",
    "    chunks_info,\n",
    "    strategy=\"base\",  # base\n",
    "    threshold=0.4,\n",
    "    k=10,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"aboba sus amogus\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding, embeddings, sources_info, chunks_info, strategy=\"base\", threshold=0.4, k=10\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestQuery(BaseModel):\n",
    "    text: str\n",
    "    relevant: bool\n",
    "    sources: list[str] | None\n",
    "\n",
    "\n",
    "def load_test_queries(validation_path: Path) -> list[TestQuery]:\n",
    "    queries: list[TestQuery] = []\n",
    "    with open(validation_path / \"queries.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            query = TestQuery.model_validate_json(line, strict=True)\n",
    "            if query.relevant:\n",
    "                queries.append(query)\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    # paths\n",
    "    meta_file_path: Path | None = None,\n",
    "    validation_path: Path | None = None,\n",
    "    texts_path: Path | None = None,\n",
    "    # sources\n",
    "    sources_info: dict[str, Source] | None = None,\n",
    "    # chunks\n",
    "    chunks_info: dict[int, Chunk] | None = None,\n",
    "    text_splitter: TextSplitter | None = None,\n",
    "    # model and embeddings\n",
    "    model: SentenceTransformer | None = None,\n",
    "    embeddings: np.ndarray | None = None,\n",
    "    # search\n",
    "    strategy: Literal[\"base\", \"majority_vote\"] = \"base\",\n",
    "    threshold: float = 0.4,\n",
    "    k: int = 10,\n",
    "    # eval part\n",
    "    metrics: list[str] | None = None,\n",
    "):\n",
    "    ####################\n",
    "    # PARAMETER CHECKS #\n",
    "    ####################\n",
    "    # perform source info extraction if not present\n",
    "    if not sources_info:\n",
    "        if not os.path.exists(meta_file_path):\n",
    "            raise ValueError(\"Unable to perform source info extraction\")\n",
    "        sources_info = load_sources_info(meta_file_path)\n",
    "\n",
    "    # perform chunking if not present\n",
    "    if not chunks_info:\n",
    "        if (\n",
    "            not os.path.exists(meta_file_path)\n",
    "            or not os.path.exists(texts_path)\n",
    "            or not isinstance(text_splitter, TextSplitter)\n",
    "        ):\n",
    "            raise ValueError(\"Unable to perform chunking\")\n",
    "        chunks_info = load_chunks_info(meta_file_path, texts_path, text_splitter)\n",
    "\n",
    "    # check provided model\n",
    "    if not isinstance(model, SentenceTransformer):\n",
    "        raise ValueError(\"Given model is not SentenceTransformer class\")\n",
    "\n",
    "    # perform chunks' texts embedding if not present\n",
    "    if embeddings is None:\n",
    "        texts = [chunk.text for chunk in chunks_info.values()]\n",
    "        embeddings = embed(texts, model, True)\n",
    "\n",
    "    ###########\n",
    "    # QUERIES #\n",
    "    ###########\n",
    "    if not os.path.exists(validation_path):\n",
    "        raise ValueError(\"Unable to find validation path\")\n",
    "\n",
    "    queries = load_test_queries(validation_path)\n",
    "\n",
    "    qrels = Qrels(name=\"queries\")\n",
    "    run = Run(name=\"queries\")\n",
    "\n",
    "    test_query_embedding = embed([query.text for query in queries], model)\n",
    "    for i, query in enumerate(queries):\n",
    "        # get results\n",
    "        results: list[SearchResult] = search(\n",
    "            test_query_embedding[i].reshape(1, -1),\n",
    "            embeddings,\n",
    "            sources_info,\n",
    "            chunks_info,\n",
    "            strategy=strategy,\n",
    "            threshold=threshold,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "        # extract ids to match ground truth ones\n",
    "        result_ids = [result.source.id for result in results]\n",
    "        if len(result_ids) == 0:\n",
    "            result_ids = [\"value that definetely is not present\"]\n",
    "\n",
    "        # add qrels (ground truth) and run (retrieved) to compare\n",
    "        qrels.add(q_id=query.text, doc_ids=query.sources, scores=[i for i in range(len(query.sources), 0, -1)])\n",
    "        run.add(q_id=query.text, doc_ids=result_ids, scores=[i for i in range(len(result_ids), 0, -1)])\n",
    "\n",
    "    return evaluate(qrels, run, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestQuery(text='verilog syntax', relevant=True, sources=['module-84616.pdf', 'module-84621.pdf', 'module-84787.pdf'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = load_test_queries(VALIDATION_PATH)\n",
    "queries[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.4}\n",
      "hit_rate@10 : 0.62\n",
      "map@10      : 0.33\n",
      "mrr@10      : 0.41\n",
      "\n",
      "{'threshold': 0.375}\n",
      "hit_rate@10 : 0.64\n",
      "map@10      : 0.34\n",
      "mrr@10      : 0.42\n",
      "\n",
      "{'threshold': 0.35}\n",
      "hit_rate@10 : 0.68\n",
      "map@10      : 0.35\n",
      "mrr@10      : 0.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [\"hits@10\", \"hit_rate@10\", \"recall@10\", \"precision@10\", \"map@10\"]\n",
    "treshold_param_grids = [\n",
    "    {\n",
    "        \"threshold\": 0.4,\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.375,\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.35,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for param_grid in treshold_param_grids:\n",
    "    metrics = evaluation(\n",
    "        meta_file_path=META_FILE_PATH,\n",
    "        validation_path=VALIDATION_PATH,\n",
    "        texts_path=TEXTS_PATH,\n",
    "        sources_info=sources_info,\n",
    "        chunks_info=chunks_info,\n",
    "        model=model,\n",
    "        embeddings=embeddings,\n",
    "        metrics=[\"hit_rate@10\", \"map@10\", \"mrr@10\"],\n",
    "        **param_grid,\n",
    "    )\n",
    "\n",
    "    print(param_grid)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:<12}: {v:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00108f85a224d849feedbe6a432066a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), 'embeddings': None}\n",
      "hit_rate@10 : 0.62\n",
      "map@10      : 0.33\n",
      "mrr@10      : 0.41\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e54982009b74ddd8cc4e0bffebfd394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), 'embeddings': None}\n",
      "hit_rate@10 : 0.70\n",
      "map@10      : 0.35\n",
      "mrr@10      : 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_minilm_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "all_mpnet_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "model_param_grids = [\n",
    "    {\n",
    "        \"model\": all_minilm_model,\n",
    "        \"embeddings\": None,\n",
    "    },\n",
    "    {\n",
    "        \"model\": all_mpnet_model,\n",
    "        \"embeddings\": None,\n",
    "    },\n",
    "]\n",
    "\n",
    "for param_grid in model_param_grids:\n",
    "    metrics = evaluation(\n",
    "        meta_file_path=META_FILE_PATH,\n",
    "        validation_path=VALIDATION_PATH,\n",
    "        texts_path=TEXTS_PATH,\n",
    "        sources_info=sources_info,\n",
    "        chunks_info=chunks_info,\n",
    "        # model=model,\n",
    "        # embeddings=embeddings,\n",
    "        metrics=[\"hit_rate@10\", \"map@10\", \"mrr@10\"],\n",
    "        **param_grid,\n",
    "    )\n",
    "\n",
    "    print(param_grid)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:<12}: {v:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-experiments-AGuved54-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
