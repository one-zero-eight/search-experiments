{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import util, SentenceTransformer, CrossEncoder\n",
    "from langchain_text_splitters import (\n",
    "    TextSplitter,\n",
    "    # RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "from ranx import Qrels, Run, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "###########\n",
    "# SOURCES #\n",
    "###########\n",
    "class Source(BaseModel):\n",
    "    id: str\n",
    "    url: str\n",
    "    name: str\n",
    "    desc: str\n",
    "    type: Literal[\"moodle\", \"file\", \"web\", \"tg\"] = Field(\"file\")\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.id.__hash__()\n",
    "\n",
    "\n",
    "class MoodleSource(Source):\n",
    "    course_id: str\n",
    "    course_url: str\n",
    "    course_name: str\n",
    "    type: Literal[\"moodle\"] = Field(\"moodle\")\n",
    "\n",
    "\n",
    "class FileSource(Source):\n",
    "    type: Literal[\"file\"] = Field(\"file\")\n",
    "\n",
    "\n",
    "class WebSource(Source):\n",
    "    type: Literal[\"web\"] = Field(\"web\")\n",
    "\n",
    "\n",
    "class TelegramSource(Source):\n",
    "    type: Literal[\"tg\"] = Field(\"tg\")\n",
    "\n",
    "\n",
    "#########\n",
    "# UTILS #\n",
    "#########\n",
    "class Chunk(BaseModel):\n",
    "    index: int = Field(ge=0)\n",
    "    source_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "##########\n",
    "# SEARCH #\n",
    "##########\n",
    "class SearchQuery(BaseModel):\n",
    "    text: str\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    source: Source\n",
    "    distance: float\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.source.id.__hash__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# meta datas\n",
    "DATA_PATH = Path(\"../data\")\n",
    "META_FILE_PATH = DATA_PATH / \"meta.json\"\n",
    "\n",
    "# text data\n",
    "TEXTS_PATH = Path(\"../texts\")\n",
    "PREPROCESSED_PATH = Path(\"../preprocessed\")\n",
    "\n",
    "# all related to validation\n",
    "VALIDATION_PATH = Path(\"../validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources_info(meta_file_path: Path) -> dict[str, Source]:\n",
    "    with open(meta_file_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "        meta_data = json.load(meta_file)\n",
    "\n",
    "    sources_info: dict[str, Source] = {}\n",
    "    for data in meta_data:\n",
    "        source: Source = Source.model_validate_json(json.dumps(data), strict=True)\n",
    "        sources_info[source.id] = source\n",
    "\n",
    "    return sources_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_info(meta_file_path: Path, texts_path: Path, text_splitter: TextSplitter) -> dict[int, Chunk]:\n",
    "    # log missing files\n",
    "    logging.basicConfig(\n",
    "        filename=\"missing.log\",\n",
    "        filemode=\"w\",\n",
    "        level=logging.INFO,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    with open(meta_file_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "        meta_data = json.load(meta_file)\n",
    "\n",
    "    # get current available sources list\n",
    "    sources: list[Source] = []\n",
    "    for data in meta_data:\n",
    "        source: Source = Source.model_validate_json(json.dumps(data), strict=True)\n",
    "        sources.append(source)\n",
    "\n",
    "    index = 0\n",
    "    chunks_info: dict[int, Chunk] = {}\n",
    "    for source in tqdm(sources, total=len(sources), desc=\"Split sources on chunks\", unit=\"source\"):\n",
    "        source_text_path = texts_path / (source.name + \".txt\")\n",
    "\n",
    "        # save not found files into logs\n",
    "        if not os.path.exists(source_text_path):\n",
    "            logging.info(source.id)\n",
    "            continue\n",
    "\n",
    "        # otherwise get their content\n",
    "        with open(source_text_path, \"r\", encoding=\"utf-8\") as text_file:\n",
    "            text = text_file.read()\n",
    "\n",
    "        # update info dict with current source's chunks\n",
    "        for chunk_text in text_splitter.split_text(text):\n",
    "            chunk = Chunk(index=index, source_id=source.id, text=chunk_text)\n",
    "            chunks_info[index] = chunk\n",
    "            index += 1\n",
    "\n",
    "    return chunks_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_by_chunk(chunk_index: int, chunks_info: dict[int, Chunk], sources_info: dict[str, Source]) -> Source:\n",
    "    if not chunks_info.get(chunk_index):\n",
    "        raise ValueError(f\"Chunk {chunk_index} not found\")\n",
    "\n",
    "    chunk: Chunk = chunks_info[chunk_index]\n",
    "\n",
    "    if not sources_info.get(chunk.source_id):\n",
    "        raise ValueError(f\"Source {chunk.source_id} not found\")\n",
    "\n",
    "    return sources_info[chunk.source_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    texts: list[str],\n",
    "    model: SentenceTransformer,\n",
    "    show_progress_bar: bool = False,\n",
    ") -> np.ndarray:\n",
    "    embeddings: np.ndarray = model.encode(texts, batch_size=64, show_progress_bar=show_progress_bar)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    query_embedding: np.ndarray,\n",
    "    embeddings: np.ndarray,\n",
    "    sources_info: dict[str, Source],\n",
    "    chunks_info: dict[int, Chunk],\n",
    "    strategy: Literal[\"base\", \"majority_vote\"] = \"base\",\n",
    "    threshold: float = 0.4,\n",
    "    top_k: int = 100,\n",
    "    k: int = 10,\n",
    ") -> list[SearchResult]:\n",
    "    \"\"\"Outputs results of semantic search with reranking strategy used among given sources.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray): vector representation of the query of size (1, {embedding_size}).\n",
    "        embeddings (np.ndarray): vector representations of a corpus.\n",
    "        chunks (list[Chunk]): little pieces of sources.\n",
    "        strategy (Literal[\"base\", \"majority_vote\"], optional): reranking strategy among found sources. Defaults to \"base\".\n",
    "        threshold (float, optional): min value of similarity to be present on candidate. Defaults to 0.4.\n",
    "        top_k (int, optional): retreive closest number of chunks. Defaults to 100.\n",
    "        k (int, optional): final maximum number of sources. Defaults to 10.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: signal that mentioned reranking strategy does not exist.\n",
    "\n",
    "    Returns:\n",
    "        list[SearchResult]: final output of sources.\n",
    "    \"\"\"\n",
    "    results = util.semantic_search(query_embedding, embeddings, top_k=top_k, score_function=util.dot_score)\n",
    "    assert len(results) == 1\n",
    "\n",
    "    chunk_indexes = []\n",
    "    search_results: list[SearchResult] = []\n",
    "    for result in results[0]:\n",
    "        chunk_index = result[\"corpus_id\"]\n",
    "        chunk_indexes.append(chunk_index)\n",
    "\n",
    "        source: Source = get_source_by_chunk(chunk_index, chunks_info, sources_info)\n",
    "        search_result = SearchResult(text=\"\", source=source, distance=result[\"score\"])\n",
    "        search_results.append(search_result)\n",
    "\n",
    "    # by distance\n",
    "    new_results: list[SearchResult] = []\n",
    "    if strategy == \"base\":\n",
    "        added_source_ids = set()\n",
    "        for search_result in search_results:\n",
    "            if search_result.distance < threshold:  # skip if lower than threshold\n",
    "                continue\n",
    "\n",
    "            if search_result.source.id not in added_source_ids:\n",
    "                added_source_ids.add(search_result.source.id)\n",
    "                new_results.append(search_result)\n",
    "\n",
    "    # count appearance of chunk's belonging to a source\n",
    "    elif strategy == \"majority_vote\":\n",
    "        # apply majority vote\n",
    "        counter = Counter(\n",
    "            [search_result.source for search_result in search_results if search_result.distance > threshold]\n",
    "        )\n",
    "        most_common = counter.most_common(10)\n",
    "\n",
    "        # filter and leave unique documents (a bit of crutch O(n^2))\n",
    "        for source, _ in most_common:\n",
    "            for result in search_results:\n",
    "                if source.id == result.source.id:\n",
    "                    new_results.append(result)\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Strategy is not supported\")\n",
    "\n",
    "    print(chunk_indexes)\n",
    "    return new_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_file(texts_path: Path, source_name: str):\n",
    "    source_path = texts_path / (source_name + \".txt\")\n",
    "    if not os.path.exists(source_path):\n",
    "        print(f\"File {source_path} not found\")\n",
    "        return\n",
    "\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "sources_info = load_sources_info(META_FILE_PATH)  # type: ignore\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# model\n",
    "# MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'  # SOTA\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import TextSplitter, RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# separators = RecursiveCharacterTextSplitter.get_separators_for_language(Language.MARKDOWN) + \\\n",
    "#              RecursiveCharacterTextSplitter.get_separators_for_language(Language.HTML)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=5000,\n",
    "#     chunk_overlap=500,\n",
    "#     len\n",
    "#     add_start_index=True,\n",
    "#     separators=separators\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Split sources on chunks: 100%|██████████| 949/949 [01:24<00:00, 11.19source/s]\n"
     ]
    }
   ],
   "source": [
    "# split on chunks\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=25, model_name=MODEL_NAME)\n",
    "chunks_info = load_chunks_info(META_FILE_PATH, PREPROCESSED_PATH, text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e8c7513536450185a8ba75dcb0968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PodYapolsky\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\search-experiments-AGuved54-py3.11\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21791, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed text chunks\n",
    "texts = [chunk.text for chunk in chunks_info.values()]\n",
    "embeddings = embed(texts, model, True)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##го содержания объявления плакаты листовки т. п. том числе на входных дверях комнат. 4. 2. 20. проживающим запрещается вносить хранить употреблять на территории комплекса алкогольные напитки кальяны наркотические вещества вносить хранить огнестрельное холодное оружие том числе топоры охотничьи ножи т. п. вз\n",
      "is a directed ij 3cycle containing vj. a directed vi vjpath of length ij followed by the directed cycle together form a directed vi vjwalk of length as desired. there are two special cases. if d. ij then followed by a directed 2cycle through vj followed by a directed 3cycle through vj constitute a directed vi vjwalk of length the 2cycle exists since and if ij then followed by a directed lcycle through vj followed by a directed 3cycle through vj constitute such a walk a real matrix is called primitive if for soe k. corollary 10. 7 the adjacency matrix a of a tournament is primitive if and only if is diconnected and v : 4. proof if is not diconnected then there are vertices vi and vj in such that vj is not reachable from vi. thus there is no directed vi vjwalk in d. it follows that the jth entry of a is zero for all kand hence a is not primitive. conversely suppose that is. diconnected. if then by theorem 10. 7 a and so a is primitive. there is just one diconnected tourna ment on three vertices figure lo. 14a and just one di\n",
      "performance and physiological function. one recent metaanalysis a statistical approach that combines data from multiple studies to draw conclusions reported that natural live high train low approaches provide the best results for enhancing performance in elite athletes while some nonelite exercisers seem to benefit from the artificial approaches. however those authors cautioned that improvements seen in these subelite athletes could have been due to a placebo effect. ethical issues have also been raised about the use of such devices. physiology of sport and exercise 6000 prevalence of acute altitude sickness prevalence of prevalence of hape hace 5000 4000 3000 2000 reported prevalence of acute altitude sickness highaltitude pulmonary edema hape and highaltitude cerebral edema hace as a function of altitude experience and in the case of acute altitude sickness rate of ascent. figure 13. 8 adapted from compiled data presented in bartsch and saltin 2008. e5149kenneywilmorefig 13. 8404559timbr3 another consequence of acute altitude sickness is an inability to sleep even if the individual is markedly fatigued. studies have shown that the inability to achieve satisfying sleep at altitude is associated with an interruption in the sleep stages. additionally some people suffer from a pattern of interrupted breathing called cheynestokes breathing which prevents them from falling\n",
      "1 doors. 23. 2. 11. 1. 1 doors within means of egress shall be in accordance with chapter unless otherwise provided in 23. 2. 11. 1. 2 through 23. 2. 11. 1. 10. 23. 2. 11. 1. 2 doors shall be permitted to be locked in accordance with the applicable use condition. 23. 2. 11. 1. 3 where egress doors are locked with keyoperated locks the provisions of 23. 7. 7 shall apply. 23. 2. 11. 1. 4 doors to resident sleeping rooms shall be not less than 28 in. 710 mm in clear width. 23. 2. 11. 1. 5 existing doors to resident sleeping rooms housing four or fewer residents shall be permitted to be not less than 19 in. 485 mm in clear width. 23. 2. 11. 1. 6 doors in a means of egress shall be permitted to be of the horizontalsliding type provided that the force necessary to slide the door to its fully open position does not exceed 50 lbf 222 where a force of 50 lbf 222 is simultaneously applied perpendicular to the door. 23. 2. 11. 1. 7 doors from areas of refuge to the exterior shall be\n",
      "##ию распорядительного акта об отчислении или выписку из него связи переводом оригинал документа при наличии предшествующем образовании также справку об обучении прошу : выдать мне на руки выдать на руки доверенному лицу доверенность прилагается 21 направить указанному лицу через операторов почтовои связи общего п\n"
     ]
    }
   ],
   "source": [
    "for text in texts[50::5000]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5142, 5116, 4974, 5059, 4847, 5143, 5085, 5014, 5083, 11415, 6000, 11111, 5078, 6003, 11427, 5084, 11107, 5058, 11346, 6006, 5080, 6482, 11349, 11871, 11407, 5070, 11437, 11943, 5062, 5081, 11364, 5054, 7909, 5109, 11371, 17890, 11353, 5082, 11511, 11330, 11283, 1466, 11360, 11542, 11963, 5016, 11106, 6009, 5140, 5022, 4774, 11378, 17811, 8757, 11412, 11339, 4795, 5023, 6478, 6976, 8821, 1585, 6205, 4997, 8381, 6991, 5066, 11347, 8788, 11300, 11516, 6008, 5144, 6001, 8391, 4956, 4772, 5053, 4906, 11399, 4775, 6002, 5055, 6004, 11509, 5087, 11859, 11340, 8566, 6965, 8557, 5108, 7151, 11401, 8779, 11390, 8576, 11109, 5544, 424]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SearchResult(source=Source(id='module-79154.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=79154', name='module-79154.pdf', desc='Graph Theory (main book)', type='moodle'), distance=0.41664445400238037),\n",
       " SearchResult(source=Source(id='module-109689.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=109689', name='module-109689.pdf', desc='Lecture Week 11 Part I (TCP Congestion Control)', type='moodle'), distance=0.38167300820350647),\n",
       " SearchResult(source=Source(id='module-92783.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92783', name='module-92783.pdf', desc=\"Lab 13. Dijkstra's algorithm\", type='moodle'), distance=0.38154342770576477),\n",
       " SearchResult(source=Source(id='module-108403.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=108403', name='module-108403.pdf', desc='Lecture Week 2 Part I (Network Characteristics)', type='moodle'), distance=0.38054201006889343),\n",
       " SearchResult(source=Source(id='module-92978.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92978', name='module-92978.pdf', desc='Lecture 14. Flow networks. Ford-Fulkerson. Max flow min cut', type='moodle'), distance=0.376510351896286),\n",
       " SearchResult(source=Source(id='module-109690.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=109690', name='module-109690.pdf', desc='Lecture Week 11 Part II (Intro to Network Layer)', type='moodle'), distance=0.37384033203125),\n",
       " SearchResult(source=Source(id='module-108416.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=108416', name='module-108416.pdf', desc='Lecture Week 1 (Introduction to Computer Networks)', type='moodle'), distance=0.37213924527168274),\n",
       " SearchResult(source=Source(id='module-92979.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92979', name='module-92979.pdf', desc='Tutorial 14. Edmonds-Karp. Max flow min cut', type='moodle'), distance=0.3664119839668274),\n",
       " SearchResult(source=Source(id='module-89466.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=89466', name='module-89466.pdf', desc='Linear Algebra and Its Applications', type='moodle'), distance=0.3632035255432129),\n",
       " SearchResult(source=Source(id='module-109407.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=109407', name='module-109407.pdf', desc='Lec 01 Networking overview (rev. 1.0)', type='moodle'), distance=0.36179235577583313)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"Burmykov Networks course lecture 11\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding, embeddings, sources_info, chunks_info, strategy=\"base\", threshold=0.35, top_k=100, k=10\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and chartrand g. 1971. introductionto the theory of graphs allyn and bacon boston harary f. cd. 1967. a seminar on graph theory holt rinehart and winston new york ore o. 1962. theory of graphs american mathematical society provi dence r. i. konig d. 1950. theorie der endlichen und unendlichen graphen chelsea new york sachs h. 1970. einfuhrung in die theorie der endlichen graphen tebner verlagsgesellsch. aft leipzig harary f. 1969. graph theory addisonwesley reading mass. berge c. 1973. graphs and hypergraphs north holland amsterdam special topics biggs n. 1974. algebraic graph theory cambridge university press cambridge tutte w. t. 1966. connectivity in graphs university of toronto press toronto ore o. 1967. the fourcolor problem academic press new york ringel g. 1974. map color theorem springerverlag berlin appendix v : suggestions for further reading 255 moon j. w. 1968. topics on tournaments holt rinehart and winston new york ford l. r. jr. and fulkerson d. r. 1962. flows in networks princeton university press princeton berge c. and ghouilahouri a.\n",
      "[-10.797634]\n",
      "affected by network delays. in order to acquire a deep understandin lets explore these delays in the context of figure 1. 16. as par routers might inform senders directly or by network delays. in order to acquire a deep understanding of packet switching and lets explore these delays in the context of figure 1. 16. as part of its endtoend computer networks we must understand the nature and im route between source and destination a packet is sent from computer networks we must understand the nature and importance of these delays. route between source and destination a packet is sent from the upstream node indirectly via receivers through router a to router b. our goal is to characterize the nodal through router a to router b. our goal is to characterize the nodal delay at router a. note that router a has an outbound link leading to router b. this note that router a has an outbound link leading to router b. this link is preceded by types of delay types of delay a queue also known as a buffer. when the packet arrives at a queue also known as a buffer. when the packet arrives at router a from the upstream node router a examines the packets header to determi lets explore these delays in the context of\n",
      "[-11.156816]\n",
      "##ize the bw by bwv inf the start vertex. bwu for widest path problem telephonecomputer network path algorithm highestbw initialize bw and bw for initialize priority queue of vertices using bw as key. while is not empty do. removemax for each vertex adjacent to and in do if min bw bw bw then bw min bw bw update in return bw widest path problemtelephone or computer network path pq widest path problemtelephone or computer network path pq widest path problemtelephone or computer network path pq widest path problemtelephone or computer network path pq coding exercise for the lab https : leetcode. comproblemsnetworkdelaytime see you next week!\n",
      "[-11.231132]\n",
      "computer networks lecture week part characteristics of computer networks artem burmyakov february 2024 packets switching mechanism address address destination host a brief classification network node types network hosts intermediate network nodes the ones that actually communicate and at the network edge source host destination host sends data receives data route data packets across a network boost signal intensity etc. routers switches bridges gateway router wifi repeater at the edge of a specific e. g. private network expends the signal coverage area characteristics of computer networks lan network e. g. at home or office a collection of devices connected together in one physical location such as a building man network connects multiple buildings or city wan network connects multiple mans or lans characteristics of computer networks lan network e. g. at home or office man network connects wan network multiple buildings or city connects multiple mans or lans a collection of devices connected together in one physical location such as a building characteristics of computer networks multiple classifications for computer networks are available characteristics of computer networks characteristics of computer networks physical network topology the layout of computer cables and other network devices e. g. hosts routers switches bridges etc. physical network topology the layout of computer cables and other network devices e. g. hosts routers switches bridges etc. switch router bridge etc. does star topology provides\n",
      "[1.5779983]\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "chunks_indexes = [\n",
    "    5142,\n",
    "    5116,\n",
    "    4974,\n",
    "    5059,\n",
    "    4847,\n",
    "    5143,\n",
    "    5085,\n",
    "    5014,\n",
    "    5083,\n",
    "    11415,\n",
    "    6000,\n",
    "    11111,\n",
    "    5078,\n",
    "    6003,\n",
    "    11427,\n",
    "    5084,\n",
    "    11107,\n",
    "    5058,\n",
    "    11346,\n",
    "    6006,\n",
    "    5080,\n",
    "    6482,\n",
    "    11349,\n",
    "    11871,\n",
    "    11407,\n",
    "    5070,\n",
    "    11437,\n",
    "    11943,\n",
    "    5062,\n",
    "    5081,\n",
    "    11364,\n",
    "    5054,\n",
    "    7909,\n",
    "    5109,\n",
    "    11371,\n",
    "    17890,\n",
    "    11353,\n",
    "    5082,\n",
    "    11511,\n",
    "    11330,\n",
    "    11283,\n",
    "    1466,\n",
    "    11360,\n",
    "    11542,\n",
    "    11963,\n",
    "    5016,\n",
    "    11106,\n",
    "    6009,\n",
    "    5140,\n",
    "    5022,\n",
    "    4774,\n",
    "    11378,\n",
    "    17811,\n",
    "    8757,\n",
    "    11412,\n",
    "    11339,\n",
    "    4795,\n",
    "    5023,\n",
    "]\n",
    "\n",
    "used_ids = set()\n",
    "filtered_chunks = []\n",
    "# print(chunks_info[chunks[1]])\n",
    "for chunk_index in chunks_indexes[:12]:\n",
    "    if chunks_info[chunk_index].source_id not in used_ids:\n",
    "        used_ids.add(chunks_info[chunk_index].source_id)\n",
    "        filtered_chunks.append(chunks_info[chunk_index])\n",
    "\n",
    "for chunk in filtered_chunks:\n",
    "    print(chunk.text)\n",
    "    print(cross_encoder.predict([(\"Burmykov Networks course lecture 11\", chunk.text)], show_progress_bar=False))\n",
    "# scores = cross_encoder.predict(\n",
    "#     [(\"Query\", \"Paragraph1\"), (\"Query\", \"Paragraph2\"), (\"Query\", \"Paragraph3\")]\n",
    "# )\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(source=Source(id='module-92034.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92034', name='module-92034.pdf', desc='Tutorial 08 - SOLID', type='moodle'), distance=0.6408908367156982),\n",
       " SearchResult(source=Source(id='module-81745.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=81745', name='module-81745.pdf', desc='Book 3 - The World Philosophy Made', type='moodle'), distance=0.3889666795730591),\n",
       " SearchResult(source=Source(id='module-84088.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=84088', name='module-84088.pdf', desc='Class 9A. Presentation', type='moodle'), distance=0.372954398393631),\n",
       " SearchResult(source=Source(id='module-92214.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=92214', name='module-92214.pdf', desc='2023 SSAD 12 Bridge, Flyweight', type='moodle'), distance=0.3594950735569),\n",
       " SearchResult(source=Source(id='module-82959.pdf', url='https://moodle.innopolis.university/mod/resource/view.php?id=82959', name='module-82959.pdf', desc='Lab 2 (bul)', type='moodle'), distance=0.3524725139141083)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"SOLID principles examples\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding,\n",
    "    embeddings,\n",
    "    sources_info,\n",
    "    chunks_info,\n",
    "    strategy=\"base\",  # base\n",
    "    threshold=0.35,\n",
    "    k=10,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SearchQuery(text=\"aboba sus amogus\")\n",
    "query_embedding = embed([query.text], model)\n",
    "\n",
    "results: list[SearchResult] = search(\n",
    "    query_embedding, embeddings, sources_info, chunks_info, strategy=\"base\", threshold=0.375, k=10\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestQuery(BaseModel):\n",
    "    text: str\n",
    "    relevant: bool\n",
    "    sources: list[str] | None\n",
    "\n",
    "\n",
    "def load_test_queries(validation_path: Path) -> list[TestQuery]:\n",
    "    queries: list[TestQuery] = []\n",
    "    with open(validation_path / \"queries.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            query = TestQuery.model_validate_json(line, strict=True)\n",
    "            if query.relevant:\n",
    "                queries.append(query)\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    # paths\n",
    "    meta_file_path: Path | None = None,\n",
    "    validation_path: Path | None = None,\n",
    "    texts_path: Path | None = None,\n",
    "    # sources\n",
    "    sources_info: dict[str, Source] | None = None,\n",
    "    # chunks\n",
    "    chunks_info: dict[int, Chunk] | None = None,\n",
    "    text_splitter: TextSplitter | None = None,\n",
    "    # model and embeddings\n",
    "    model: SentenceTransformer | None = None,\n",
    "    embeddings: np.ndarray | None = None,\n",
    "    # search\n",
    "    strategy: Literal[\"base\", \"majority_vote\"] = \"base\",\n",
    "    threshold: float = 0.4,\n",
    "    k: int = 10,\n",
    "    # eval part\n",
    "    metrics: list[str] | None = None,\n",
    "):\n",
    "    ####################\n",
    "    # PARAMETER CHECKS #\n",
    "    ####################\n",
    "    # perform source info extraction if not present\n",
    "    if not sources_info:\n",
    "        if not os.path.exists(meta_file_path):\n",
    "            raise ValueError(\"Unable to perform source info extraction\")\n",
    "        sources_info = load_sources_info(meta_file_path)\n",
    "\n",
    "    # perform chunking if not present\n",
    "    if not chunks_info:\n",
    "        if (\n",
    "            not os.path.exists(meta_file_path)\n",
    "            or not os.path.exists(texts_path)\n",
    "            or not isinstance(text_splitter, TextSplitter)\n",
    "        ):\n",
    "            raise ValueError(\"Unable to perform chunking\")\n",
    "        chunks_info = load_chunks_info(meta_file_path, texts_path, text_splitter)\n",
    "\n",
    "    # check provided model\n",
    "    if not isinstance(model, SentenceTransformer):\n",
    "        raise ValueError(\"Given model is not SentenceTransformer class\")\n",
    "\n",
    "    # perform chunks' texts embedding if not present\n",
    "    if embeddings is None:\n",
    "        texts = [chunk.text for chunk in chunks_info.values()]\n",
    "        embeddings = embed(texts, model, True)\n",
    "\n",
    "    ###########\n",
    "    # QUERIES #\n",
    "    ###########\n",
    "    if not os.path.exists(validation_path):\n",
    "        raise ValueError(\"Unable to find validation path\")\n",
    "\n",
    "    queries = load_test_queries(validation_path)\n",
    "\n",
    "    qrels = Qrels(name=\"queries\")\n",
    "    run = Run(name=\"queries\")\n",
    "\n",
    "    test_query_embedding = embed([query.text for query in queries], model)\n",
    "    for i, query in enumerate(queries):\n",
    "        # get results\n",
    "        results: list[SearchResult] = search(\n",
    "            test_query_embedding[i].reshape(1, -1),\n",
    "            embeddings,\n",
    "            sources_info,\n",
    "            chunks_info,\n",
    "            strategy=strategy,\n",
    "            threshold=threshold,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "        # extract ids to match ground truth ones\n",
    "        result_ids = [result.source.id for result in results]\n",
    "        if len(result_ids) == 0:\n",
    "            result_ids = [\"value that definetely is not present\"]\n",
    "\n",
    "        # add qrels (ground truth) and run (retrieved) to compare\n",
    "        qrels.add(q_id=query.text, doc_ids=query.sources, scores=[i for i in range(len(query.sources), 0, -1)])\n",
    "        run.add(q_id=query.text, doc_ids=result_ids, scores=[i for i in range(len(result_ids), 0, -1)])\n",
    "\n",
    "    return evaluate(qrels, run, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestQuery(text='verilog syntax', relevant=True, sources=['module-84616.pdf', 'module-84621.pdf', 'module-84787.pdf'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = load_test_queries(VALIDATION_PATH)\n",
    "queries[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.4}\n",
      "hit_rate@10 : 0.62\n",
      "map@10      : 0.33\n",
      "mrr@10      : 0.41\n",
      "\n",
      "{'threshold': 0.375}\n",
      "hit_rate@10 : 0.64\n",
      "map@10      : 0.34\n",
      "mrr@10      : 0.42\n",
      "\n",
      "{'threshold': 0.35}\n",
      "hit_rate@10 : 0.68\n",
      "map@10      : 0.35\n",
      "mrr@10      : 0.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [\"hits@10\", \"hit_rate@10\", \"recall@10\", \"precision@10\", \"map@10\"]\n",
    "treshold_param_grids = [\n",
    "    {\n",
    "        \"threshold\": 0.4,\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.375,\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.35,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for param_grid in treshold_param_grids:\n",
    "    metrics = evaluation(\n",
    "        meta_file_path=META_FILE_PATH,\n",
    "        validation_path=VALIDATION_PATH,\n",
    "        texts_path=TEXTS_PATH,\n",
    "        sources_info=sources_info,\n",
    "        chunks_info=chunks_info,\n",
    "        model=model,\n",
    "        embeddings=embeddings,\n",
    "        metrics=[\"hit_rate@10\", \"map@10\", \"mrr@10\"],\n",
    "        **param_grid,\n",
    "    )\n",
    "\n",
    "    print(param_grid)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:<12}: {v:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00108f85a224d849feedbe6a432066a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), 'embeddings': None}\n",
      "hit_rate@10 : 0.62\n",
      "map@10      : 0.33\n",
      "mrr@10      : 0.41\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e54982009b74ddd8cc4e0bffebfd394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      "), 'embeddings': None}\n",
      "hit_rate@10 : 0.70\n",
      "map@10      : 0.35\n",
      "mrr@10      : 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_minilm_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "all_mpnet_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "model_param_grids = [\n",
    "    {\n",
    "        \"model\": all_minilm_model,\n",
    "        \"embeddings\": None,\n",
    "    },\n",
    "    {\n",
    "        \"model\": all_mpnet_model,\n",
    "        \"embeddings\": None,\n",
    "    },\n",
    "]\n",
    "\n",
    "for param_grid in model_param_grids:\n",
    "    metrics = evaluation(\n",
    "        meta_file_path=META_FILE_PATH,\n",
    "        validation_path=VALIDATION_PATH,\n",
    "        texts_path=TEXTS_PATH,\n",
    "        sources_info=sources_info,\n",
    "        chunks_info=chunks_info,\n",
    "        # model=model,\n",
    "        # embeddings=embeddings,\n",
    "        metrics=[\"hit_rate@10\", \"map@10\", \"mrr@10\"],\n",
    "        **param_grid,\n",
    "    )\n",
    "\n",
    "    print(param_grid)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:<12}: {v:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-experiments-AGuved54-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
