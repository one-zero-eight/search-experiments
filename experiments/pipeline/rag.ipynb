{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO -> (pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minio = \"^7.2.7\"\n",
    "sentence-transformers = \"^3.0.1\"\n",
    "faiss-cpu = \"^1.8.0.post1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install minio image and run its created container  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quay.io/minio/minio:latest\n"
     ]
    }
   ],
   "source": [
    "!docker pull -q quay.io/minio/minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3e67b18f774a8a2e5f6cec203170ee28c2053dc3b8578ab3572e124a6aaadd4\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 9000:9000 -p 9001:9001 --name minio -e \"MINIO_ROOT_USER=minioadmin\" -e \"MINIO_ROOT_PASSWORD=password\" -v D:\\data:/data quay.io/minio/minio server /data --console-address \":9001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connenct to the minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio, S3Error\n",
    "\n",
    "# create client with access key and secret key with specific region.\n",
    "client = Minio(\n",
    "    endpoint=\"127.0.0.1:9000\",  # 9090\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmight be a solution to search api problem\\nCreate client with custom HTTP client using proxy server.\\n# import urllib3\\n# client = Minio(\\n#     \"SERVER:PORT\",\\n#     access_key=\"ACCESS_KEY\",\\n#     secret_key=\"SECRET_KEY\",\\n#     secure=True,\\n#     http_client=urllib3.ProxyManager(\\n#         \"https://PROXYSERVER:PROXYPORT/\",\\n#         timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\\n#         cert_reqs=\"CERT_REQUIRED\",\\n#         retries=urllib3.Retry(\\n#             total=5,\\n#             backoff_factor=0.2,\\n#             status_forcelist=[500, 502, 503, 504],\\n#         ),\\n#     ),\\n# )\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "might be a solution to search api problem\n",
    "Create client with custom HTTP client using proxy server.\n",
    "# import urllib3\n",
    "# client = Minio(\n",
    "#     \"SERVER:PORT\",\n",
    "#     access_key=\"ACCESS_KEY\",\n",
    "#     secret_key=\"SECRET_KEY\",\n",
    "#     secure=True,\n",
    "#     http_client=urllib3.ProxyManager(\n",
    "#         \"https://PROXYSERVER:PROXYPORT/\",\n",
    "#         timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n",
    "#         cert_reqs=\"CERT_REQUIRED\",\n",
    "#         retries=urllib3.Retry(\n",
    "#             total=5,\n",
    "#             backoff_factor=0.2,\n",
    "#             status_forcelist=[500, 502, 503, 504],\n",
    "#         ),\n",
    "#     ),\n",
    "# )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Bucket('test')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME = \"test\"\n",
    "\n",
    "if not client.bucket_exists(BUCKET_NAME):\n",
    "    client.make_bucket(BUCKET_NAME)\n",
    "\n",
    "client.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<minio.datatypes.Object object at 0x0000021D27DE0690>\n"
     ]
    }
   ],
   "source": [
    "objects = client.list_objects(BUCKET_NAME)\n",
    "for obj in objects:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'module-111237.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copyfile(\"../../data/moodle/files/module-111237.pdf\", \"module-111237.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    file_path = \"module-111237.pdf\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        file_stat = os.stat(file_path)\n",
    "\n",
    "        client.put_object(\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            object_name=file_path,\n",
    "            data=file,\n",
    "            length=file_stat.st_size,\n",
    "            content_type=\"application/pdf\",\n",
    "        )\n",
    "except S3Error as e:\n",
    "    print(\"error occurred.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<minio.datatypes.Object object at 0x0000021D28304690>\n"
     ]
    }
   ],
   "source": [
    "objects = client.list_objects(BUCKET_NAME)\n",
    "for obj in objects:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of an object.\n",
    "try:\n",
    "    client.fget_object(BUCKET_NAME, file_path, file_path)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<minio.datatypes.Object object at 0x0000021D27D99590>\n"
     ]
    }
   ],
   "source": [
    "objects = client.list_objects(BUCKET_NAME)\n",
    "for obj in objects:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pymupdf4llm -> (md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pymupdf4llm import process_document, join_chunks\n",
    "\n",
    "entries = [f for f in os.listdir(\".\") if f.endswith(\".pdf\")]\n",
    "processed_documents = [process_document(entry) for entry in entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8f7ab7757f4d0984fc2e20491aaf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = []\n",
    "for result in tqdm(processed_documents, total=len(entries)):\n",
    "    texts.append(join_chunks(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<h2><b>Deep Learning for Search</b></h2>\\n<h2><b>(summer 2024)</b></h2>\\n\\nAlbert Nasybullin\\n\\n-----\\n\\n‚óè Masters in Robotics and Computer\\nVision, Innopolis (2021)\\n\\n‚óè PhD student, Deep Learning and\\nBrain-Computer Interfaces\\n\\n‚óè Teaching: Probability & Statistics,\\nDigital Signal Processing, Machine\\nLearning, Deep Learning (Innopolis\\nUniversity, SPSU)\\n\\n‚óè Machine Learning for Banking\\n\\n‚óè Nowadays, MTS. Machine Learning\\nEngineer (NLP & Search)\\n tlg: @levshaazz\\nemail: levshaazz@gmail.com\\n\\n-----\\n\\n<b>The Course Aims</b>\\n\\n‚óè Natural Language Processing\\n\\n‚óè Similarity Search\\n\\n‚óè Deep Learning\\n\\n‚óè Machine Learning System Design\\n\\n‚óè Metrics of Search\\n\\n‚óè Risk approach to Machine Learning\\n\\n‚óè Optimization of Neural Nets and Data\\nstorage\\n\\n‚óè Data quality and augmentation\\n\\n‚óè Retrieval Augmented Generation\\n\\n-----\\n\\n<b>The Course Aims</b>\\n\\n‚óè Natural Language Processing\\n\\n‚óè Similarity Search\\n\\n‚óè Deep Learning\\n\\n‚óè Machine Learning System Design\\n\\n‚óè Metrics of Search\\n\\n‚óè Risk approach to Machine Learning\\n\\n‚óè Optimization of Neural Nets and Data\\nstorage\\n\\n‚óè Data quality and augmentation\\n\\n‚óè Retrieval Augmented Generation\\n\\n<b>By the end of the course you will have to be able to design and develop Deep</b>\\n<b>Learning-based search application</b>\\n\\n-----\\n\\n<b>The Course Highlights</b>\\n\\n‚óè Tuesdays, 17:40 ‚Äì 20:50 (314)\\n\\n‚óè Start: June 4\\n\\n‚óè End: July 16\\n\\n‚óè Midterm exam: June 25 (20%)\\n\\n‚óè Final exam: July 16 (30%)\\n\\n‚óè Project defense: July 16 (20%)\\n\\n‚óè ‚ÄúLecture + Lab‚Äù or ‚ÄúLecture + Lecture‚Äù\\nformat\\n\\n‚óè In class labs are to be graded (3 * 5%)\\n\\n‚óè Written home assignments (3 * 5%)\\n\\n‚óè The course long team projects (up to 3\\npeople)\\n\\n<b>I expect a lot of collaboration from your side!!!</b>\\n\\n-----\\n\\n\\n\\n-----\\n\\n<b>‚ÄúI will make your life miserable‚Äù</b>\\n<b>Prof. Adil Khan</b>\\n\\n-----\\n\\n<h2><b>Introduction to Search</b></h2>\\n<h2><b>(and a little recap)</b></h2>\\n\\n-----\\n\\nAll of that is search (+ a little bit of RecSys)\\n\\n-----\\n\\n<b>Why to make search better?</b>\\n\\n-----\\n\\n<b>Why to make search better?</b>\\n\\n-----\\n\\n<b>Simplest perspective</b>\\n\\n-----\\n\\n<b>Simplest perspective</b>\\n\\n<b>How?</b> <b>How?</b> <b>How?</b>\\n\\n-----\\n\\n\\n\\n-----\\n\\n\\n\\n-----\\n\\n<b>Basics of Natural Language Processing</b>\\n\\n‚óè Tokenization\\n\\n‚óè Stemming\\n\\n‚óè Lemmatization\\n\\n‚óè Ngrams\\n\\n‚óè Stop-words\\n\\n‚óè Regular expressions\\n\\n‚óè Bag of words\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\nCore libs:\\n\\n<b>‚óè</b> <b>NLTK</b>\\n\\n‚óè TextBlob\\n\\n‚óè spaCy\\n\\n‚óè Gensim\\n\\n‚óè PyTorch\\n\\n‚óè Keras\\n<b>Challenges:</b>\\n\\n1 arabic word = 6 meanings in english\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n<b>Sentence tokenization</b>\\n<b>Word tokenization</b>\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n<b>Punctuation tokenization</b>\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n<b>Treebank Word tokenization</b>\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n<b>Tweet tokenization</b>\\n\\n-----\\n\\n<b>Tokenization</b>\\n\\n<b>MWET tokenization (multiple word expressions)</b>\\n\\n-----\\n\\n<b>Stemming & Lemmatization</b>\\n\\nBoth are normalization techniques in Natural Language Processing\\n\\n<b>Stemming:</b>\\n\\na crude heuristic process that cuts off the ‚Äúextra‚Äù from the root of words, often resulting in the loss of\\nword-forming suffixes.\\n\\n<b>Lemmatization</b>\\n\\na more subtle process that uses dictionary and morphological analysis to eventually bring a word to\\nits canonical form, the lemma.\\n\\n-----\\n\\n<b>Stemming & Lemmatization</b>\\n\\n-----\\n\\n<b>Stemming & Lemmatization</b>\\n<b>‚Äúseen‚Äù and ‚Äúdrove‚Äù</b>\\n\\n-----\\n\\n<b>Ngrams</b>\\n\\n<b>Ngrams:</b>\\n contiguous sequences of n words. For example\\n‚Äúriverbank‚Äù,‚Äù The three musketeers‚Äù etc.If the\\nnumber of words is two, it is called bigram. For 3\\nwords it is called a trigram and so on.\\n\\n-----\\n\\n<b>Ngrams</b>\\n\\n-----\\n\\n<b>Stop-words</b>\\n\\n<b>Stop-words:</b>\\n\\n-----\\n\\n<b>Stop-words</b>\\n\\n-----\\n\\n<b>Regular expressions</b>\\n<b>RegEx:</b>\\n\\n(regular, regexp, regex) is a sequence of\\ncharacters that defines a search pattern\\n\\n‚óè . - any character except line feed;\\n\\n‚óè \\\\w - one character;\\n\\n‚óè \\\\d - one digit;\\n\\n‚óè \\\\s - one space;\\n\\n‚óè \\\\W - one non-character;\\n\\n‚óè \\\\D - one non-digit;\\n\\n‚óè \\\\S - one non-space;\\n\\n‚óè [abc] - finds any of the specified characters match\\nany of a, b, or c;\\n\\n‚óè [^abc] - finds any character other than the\\nspecified ones;\\n\\n‚óè [a-g] - finds a character between a and g.\\n\\n-----\\n\\nThe re module in Python represents regular expression\\noperations. We can use the re.sub function to replace\\nanything that fits the search pattern with the specified\\nstring. This is how to replace all non-words with spaces\\n<b>Regular expressions</b>\\n\\n-----\\n\\nThe re module in Python represents regular expression\\noperations. We can use the re.sub function to replace\\nanything that fits the search pattern with the specified\\nstring. This is how to replace all non-words with spaces\\n<b>Regular expressions</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n<b>Bag of words:</b>\\n\\n‚óè Machine learning algorithms cannot deal directly with raw text\\n\\n‚óè necessary to convert text into sets of numbers (vectors), aka ‚Äúextract features‚Äù\\n\\n‚óè simple feature extraction technique for text mining\\n\\n‚óè describes the occurrences of each word in the text.\\n\\nTo use the model, we need to:\\n\\n‚óè Define a vocabulary of known words (tokens).\\n\\n‚óè Select the degree of occurrence of known words.\\n\\nIntuition tells us that <b>similar documents</b> have <b>similar content</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n<b>Bag of words:</b>\\n\\nSometimes ‚ÄúBag of words‚Äù produces sparse matrices. <b>How to fix?</b>\\n\\n-----\\n\\n<b>Bag of words</b>\\n\\n<b>Bag of words:</b>\\n\\nSometimes ‚ÄúBag of words‚Äù produces sparse matrices. <b>How to fix?</b>\\n\\n‚óè ignore word case\\n\\n‚óè ignore punctuation\\n\\n‚óè throw out stop words\\n\\n‚óè bring words back to their basic forms (lemmatization and stemming)\\n\\n‚óè correct misspelled words\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n‚óè documents are not a sequence of words\\n\\n‚óè documents are points in a <b>multi-dimensional vector space</b>\\n\\n‚óè each vector has the <b>same length</b>\\n\\n‚óè now we can measure <b>the distance</b>\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n‚óè documents are not a sequence of words\\n\\n‚óè documents are points in a <b>multi-dimensional vector space</b>\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n‚óè cosine similarity\\n\\n‚óè cosine of the angle between any two points (more precisely their vectors starting\\nfrom the origin)\\n\\n‚óè closer the score 1, the smaller the angle between the vectors and the more\\nsimilar the documents are\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n-----\\n\\n<b>Bag of words & Similarity</b>\\n\\n-----\\n\\n<b>Home Assignment 1</b>\\n\\n‚óè Deadline: June 10, 23:59\\n\\n‚óè Task:\\n‚óã Read the paper ‚ÄúDistributed Representations of Words and Phrases and\\ntheir Compositionality‚Äù [ <a href='https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf'>link</a> ]\\n‚óã Summarize main concepts in essay (2 pages min)\\n‚óã Take example at slides 34-44 (Bag of Words & Similarity) and repeat it with\\nyour own hands (written form, no code)\\n‚óã Submit\\n\\n-----\\n\\n<b>Questions?</b>\\n\\n-----\\n\\n<b>Brake (20 minutes)</b>\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess -> (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer  # , TreebankWordTokenizer\n",
    "import unicodedata\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "\n",
    "def to_lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \")\n",
    "\n",
    "\n",
    "def remove_markdown(text: str) -> str:\n",
    "    # TODO: ...\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    return re.sub(url_pattern, \"\", text)\n",
    "\n",
    "\n",
    "def remove_stop_words(words: list[str]) -> list[str]:\n",
    "    stop_words = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "    filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocessing():\n",
    "    assert normalize(\"ùëÉùëúùë†ùëñùë°ùëñùë£ùëíùë†\") == \"Positives\"\n",
    "    assert to_lowercase(\"Omg Hiiiii\") == \"omg hiiiii\"\n",
    "    assert remove_html(\"<h2><b>Deep Learning for Search</b></h2>\") == \"Deep Learning for Search\"\n",
    "    assert remove_punctuation(\"Hello, quite! A lot? :_+\") == \"Hello quite A lot _\"\n",
    "    # assert remove_markdown(\"\") == \"\"\n",
    "    assert remove_urls(\"Here is the link https://hotel.innopolis.university/dokumenty/ !\") == \"Here is the link  !\"\n",
    "    # assert remove_stop_words([]) == []\n",
    "\n",
    "\n",
    "test_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str, tokenizer) -> str:\n",
    "    text = normalize(text)  # . normalize\n",
    "    text = to_lowercase(text)  # to lower case (think of Names etc)\n",
    "    text = remove_html(text)  # remove tags\n",
    "    text = remove_punctuation(text)  # remove punctuation\n",
    "    # text = remove_markdown(text)\n",
    "    text = remove_urls(text)  # remove urls\n",
    "\n",
    "    # TOKENIZE #\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words = remove_stop_words(words)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [preprocess(text, WordPunctTokenizer()) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep learning search summer 2024 albert nasybullin masters robotics computer vision innopolis 2021 phd student deep learning braincomputer interfaces teaching probability statistics digital signal processing machine learning deep learning innopolis university spsu machine learning banking nowadays mts machine learning engineer nlp search tlg levshaazz email levshaazzgmailcom course aims natural language processing similarity search deep learning machine learning system design metrics search risk approach machine learning optimization neural nets data storage data quality augmentation retrieval augmented generation course aims natural language processing similarity search deep learning machine learning system design metrics search risk approach machine learning optimization neural nets data storage data quality augmentation retrieval augmented generation end course able design develop deep learningbased search application course highlights tuesdays 1740 2050 314 start june 4 end july 16 midterm exam june 25 20 final exam july 16 30 project defense july 16 20 lecture lab lecture lecture format class labs graded 3 5 written home assignments 3 5 course long team projects 3 people expect lot collaboration side make life miserable prof adil khan introduction search little recap search little bit recsys make search better make search better simplest perspective simplest perspective basics natural language processing tokenization stemming lemmatization ngrams stopwords regular expressions bag words tokenization core libs nltk textblob spacy gensim pytorch keras challenges 1 arabic word 6 meanings english tokenization sentence tokenization word tokenization tokenization tokenization punctuation tokenization tokenization treebank word tokenization tokenization tweet tokenization tokenization mwet tokenization multiple word expressions stemming lemmatization normalization techniques natural language processing stemming crude heuristic process cuts extra root words often resulting loss wordforming suffixes lemmatization subtle process uses dictionary morphological analysis eventually bring word canonical form lemma stemming lemmatization stemming lemmatization seen drove ngrams ngrams contiguous sequences n words example riverbank three musketeers etcif number words two called bigram 3 words called trigram ngrams stopwords stopwords stopwords regular expressions regex regular regexp regex sequence characters defines search pattern character except line feed w one character one digit one space w one noncharacter one nondigit one nonspace abc finds specified characters match b c abc finds character specified ones ag finds character g module python represents regular expression operations use resub function replace anything fits search pattern specified string replace nonwords spaces regular expressions module python represents regular expression operations use resub function replace anything fits search pattern specified string replace nonwords spaces regular expressions bag words bag words machine learning algorithms cannot deal directly raw text necessary convert text sets numbers vectors aka extract features simple feature extraction technique text mining describes occurrences word text use model need define vocabulary known words tokens select degree occurrence known words intuition tells us similar documents similar content bag words bag words bag words bag words bag words sometimes bag words produces sparse matrices fix bag words bag words sometimes bag words produces sparse matrices fix ignore word case ignore punctuation throw stop words bring words back basic forms lemmatization stemming correct misspelled words bag words similarity documents sequence words documents points multidimensional vector space vector length measure distance bag words similarity documents sequence words documents points multidimensional vector space bag words similarity cosine similarity cosine angle two points precisely vectors starting origin closer score 1 smaller angle vectors similar documents bag words similarity bag words similarity bag words similarity home assignment 1 deadline june 10 2359 task read paper distributed representations words phrases compositionality link summarize main concepts essay 2 pages min take example slides 3444 bag words similarity repeat hands written form code submit questions brake 20 minutes'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junking -> (junks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deep learning search summer 2024 albert nasybullin masters robotics computer vision innopolis 2021 phd student deep learning braincomputer interfaces',\n",
       " 'interfaces teaching probability statistics digital signal processing machine learning deep learning innopolis university spsu machine learning',\n",
       " 'machine learning banking nowadays mts machine learning engineer nlp search tlg levshaazz email levshaazzgmailcom course aims natural language',\n",
       " 'natural language processing similarity search deep learning machine learning system design metrics search risk approach machine learning optimization',\n",
       " 'optimization neural nets data storage data quality augmentation retrieval augmented generation course aims natural language processing similarity',\n",
       " 'similarity search deep learning machine learning system design metrics search risk approach machine learning optimization neural nets data storage',\n",
       " 'nets data storage data quality augmentation retrieval augmented generation end course able design develop deep learningbased search application',\n",
       " 'search application course highlights tuesdays 1740 2050 314 start june 4 end july 16 midterm exam june 25 20 final exam july 16 30 project defense',\n",
       " '30 project defense july 16 20 lecture lab lecture lecture format class labs graded 3 5 written home assignments 3 5 course long team projects 3',\n",
       " 'team projects 3 people expect lot collaboration side make life miserable prof adil khan introduction search little recap search little bit recsys',\n",
       " 'little bit recsys make search better make search better simplest perspective simplest perspective basics natural language processing tokenization',\n",
       " 'tokenization stemming lemmatization ngrams stopwords regular expressions bag words tokenization core libs nltk textblob spacy gensim pytorch keras',\n",
       " 'pytorch keras challenges 1 arabic word 6 meanings english tokenization sentence tokenization word tokenization tokenization tokenization punctuation',\n",
       " 'punctuation tokenization tokenization treebank word tokenization tokenization tweet tokenization tokenization mwet tokenization multiple word',\n",
       " 'multiple word expressions stemming lemmatization normalization techniques natural language processing stemming crude heuristic process cuts extra',\n",
       " 'process cuts extra root words often resulting loss wordforming suffixes lemmatization subtle process uses dictionary morphological analysis',\n",
       " 'analysis eventually bring word canonical form lemma stemming lemmatization stemming lemmatization seen drove ngrams ngrams contiguous sequences n',\n",
       " 'sequences n words example riverbank three musketeers etcif number words two called bigram 3 words called trigram ngrams stopwords stopwords stopwords',\n",
       " 'stopwords stopwords regular expressions regex regular regexp regex sequence characters defines search pattern character except line feed w one',\n",
       " 'line feed w one character one digit one space w one noncharacter one nondigit one nonspace abc finds specified characters match b c abc finds',\n",
       " 'match b c abc finds character specified ones ag finds character g module python represents regular expression operations use resub function replace',\n",
       " 'function replace anything fits search pattern specified string replace nonwords spaces regular expressions module python represents regular',\n",
       " 'represents regular expression operations use resub function replace anything fits search pattern specified string replace nonwords spaces regular',\n",
       " 'spaces regular expressions bag words bag words machine learning algorithms cannot deal directly raw text necessary convert text sets numbers vectors',\n",
       " 'numbers vectors aka extract features simple feature extraction technique text mining describes occurrences word text use model need define vocabulary',\n",
       " 'define vocabulary known words tokens select degree occurrence known words intuition tells us similar documents similar content bag words bag words',\n",
       " 'bag words bag words bag words bag words bag words sometimes bag words produces sparse matrices fix bag words bag words sometimes bag words produces',\n",
       " 'bag words produces sparse matrices fix ignore word case ignore punctuation throw stop words bring words back basic forms lemmatization stemming',\n",
       " 'stemming correct misspelled words bag words similarity documents sequence words documents points multidimensional vector space vector length measure',\n",
       " 'length measure distance bag words similarity documents sequence words documents points multidimensional vector space bag words similarity cosine',\n",
       " 'similarity cosine similarity cosine angle two points precisely vectors starting origin closer score 1 smaller angle vectors similar documents bag',\n",
       " 'documents bag words similarity bag words similarity bag words similarity home assignment 1 deadline june 10 2359 task read paper distributed',\n",
       " 'paper distributed representations words phrases compositionality link summarize main concepts essay 2 pages min take example slides 3444 bag words',\n",
       " '3444 bag words similarity repeat hands written form code submit questions brake 20 minutes']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings -> (vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PodYapolsky\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\search-experiments-AGuved54-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# MODEL_NAME = 'all-mpnet-base-v2'  # SOTA\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PodYapolsky\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\search-experiments-AGuved54-py3.11\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 384)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for normalization of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = 0\n",
    "for embedding in embeddings:\n",
    "    if abs(np.linalg.norm(embeddings[1]) - 1) > 1e-5:\n",
    "        i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since vectors are normalized, IP (Inner Product) will work as cosine\n",
    "\n",
    "https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n",
    "https://github.com/facebookresearch/faiss/issues/95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "n, d = embeddings.shape\n",
    "index = faiss.IndexFlatIP(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode([\"Phd student Albert Nasibullin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(query_embedding, 3)  # noqa: E741"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First chunk is the closest as we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36785334 0.21152827 0.1874401 ]] [[0 7 2]]\n"
     ]
    }
   ],
   "source": [
    "print(D, I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-experiments-AGuved54-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
