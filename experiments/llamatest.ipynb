{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:43:37.556199Z",
     "start_time": "2024-06-09T16:43:35.128167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama.pull(\"mxbai-embed-large\")  # https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\",\n",
    "                                     embed_instruction=\"Represent this sentence for searching relevant passages: \",\n",
    "                                     query_instruction=\"Represent this sentence for searching relevant passages: \")"
   ],
   "id": "dfac9d02c4073fb9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:43:46.981021Z",
     "start_time": "2024-06-09T16:43:45.819785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymupdf\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pymupdf.TEXT_PRESERVE_LIGATURES = False\n",
    "pymupdf.TEXT_DEHYPHENATE = True\n",
    "\n",
    "attention_pdf = PyMuPDFLoader(\"attention.pdf\")\n",
    "transformer_pdf = PyMuPDFLoader(\"transformer++.pdf\")\n",
    "distributed_pdf = PyMuPDFLoader(\"distributed-repr.pdf\")"
   ],
   "id": "bc5bb79c597ab31a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:44:05.025536Z",
     "start_time": "2024-06-09T16:44:03.702113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "api_key = \"8182bd49c9b34c76d6b5526827c2374dac5bcbe425026b1a4b50aa8ccdcae8c7\"\n",
    "url = \"http://localhost:6333\"\n",
    "\n",
    "# noinspection PyTypeChecker\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    "    url=url,\n",
    "    path=None,\n",
    "    embedding=ollama_embeddings,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"pdfs\"\n",
    ")\n",
    "qdrant_receiver = qdrant.as_retriever()"
   ],
   "id": "9afe7301d3998ece",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dante/.cache/pypoetry/virtualenvs/innohassle-search-experiments-3GmagOUg-py3.12/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n",
      "/home/dante/.cache/pypoetry/virtualenvs/innohassle-search-experiments-3GmagOUg-py3.12/lib/python3.12/site-packages/qdrant_client/async_qdrant_remote.py:119: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:44:11.096526Z",
     "start_time": "2024-06-09T16:44:09.379180Z"
    }
   },
   "cell_type": "code",
   "source": "found_docs = qdrant_receiver.invoke(\"What is attention mechanism?\")",
   "id": "a14c090498146299",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:44:12.291097Z",
     "start_time": "2024-06-09T16:44:12.286418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_to_qdrant():\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    from qdrant_client import models\n",
    "\n",
    "    # delete all\n",
    "    qdrant.delete(models.Filter())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "    loaded = attention_pdf.load() + transformer_pdf.load() + distributed_pdf.load()\n",
    "    all_split_docs = text_splitter.split_documents(loaded)\n",
    "    # add to qdrant\n",
    "    qdrant.add_documents(all_split_docs)"
   ],
   "id": "db6e49fd03dd779e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:44:19.363690Z",
     "start_time": "2024-06-09T16:44:18.248602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain import hub\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "\n",
    "llm_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\"context\": qdrant_receiver, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=llm_chain)"
   ],
   "id": "928d3eb100b4b28e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:44:43.256374Z",
     "start_time": "2024-06-09T16:44:20.962022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = dict()\n",
    "curr_key = None\n",
    "for chunk in rag_chain.stream(\"What it self-attention mechanism in transformers?\"):\n",
    "    for key in chunk:\n",
    "        if key not in output:\n",
    "            output[key] = chunk[key]\n",
    "        else:\n",
    "            output[key] += chunk[key]\n",
    "        if key != curr_key:\n",
    "            if isinstance(chunk[key], list):\n",
    "                if not chunk[key]:\n",
    "                    print(f\"\\n\\n{key}: []\", end=\"\", flush=True)\n",
    "                else:\n",
    "                    print(f\"\\n\\n{key}: [{type(chunk[key][0])}] len {len(chunk[key])}\", end=\"\", flush=True)\n",
    "            else:\n",
    "                print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk[key], end=\"\", flush=True)\n",
    "        curr_key = key"
   ],
   "id": "9dab4a9a9460f83b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question: What it self-attention mechanism in transformers?\n",
      "\n",
      "context: [<class 'langchain_core.documents.base.Document'>] len 4\n",
      "\n",
      "answer: [/INST] The self-attention mechanism in transformers allows each token in a sequence to attend to every other token and weigh their importance, helping the model understand relationships between tokens. This is different from traditional recurrent neural networks (RNNs) that only consider sequential information. By allowing tokens to attend to each other, self-attention enables the model to capture long-range dependencies and context. [/INST]"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T16:45:10.898918Z",
     "start_time": "2024-06-09T16:45:10.893080Z"
    }
   },
   "cell_type": "code",
   "source": "output",
   "id": "b612f9efcdc599d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What it self-attention mechanism in transformers?',\n",
       " 'context': [Document(page_content='We employ three types of regularization during training:\\n7', metadata={'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': 'attention.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 6, 'producer': 'pdfTeX-1.40.25', 'source': 'attention.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '', '_id': 'afa40854-8d2e-43cb-baed-90eae89b5aab', '_collection_name': 'pdfs'}),\n",
       "  Document(page_content='arXiv:2003.04974v1  [cs.CL]  2 Mar 2020', metadata={'author': '', 'creationDate': 'D:20200402090304Z', 'creator': 'LaTeX with hyperref package', 'file_path': 'transformer++.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200402090304Z', 'page': 0, 'producer': 'pdfTeX-1.40.17', 'source': 'transformer++.pdf', 'subject': '', 'title': '', 'total_pages': 7, 'trapped': '', '_id': '36a7b9c8-5fe5-4bb9-9be3-b4b11636890a', '_collection_name': 'pdfs'}),\n",
       "  Document(page_content='learning high-quality distributed vector representations that capture a large num-\\nber of precise syntactic and semantic word relationships. In this paper we present\\nseveral extensions that improve both the quality of the vectors and the training\\nspeed. By subsampling of the frequent words we obtain signiÔ¨Åcant speedup and\\nalso learn more regular word representations. We also describe a simple alterna-\\ntive to the hierarchical softmax called negative sampling.', metadata={'author': 'kaichen', 'creationDate': 'Wed May  1 15:07:42 2013', 'creator': 'gnuplot 4.4 patchlevel 3', 'file_path': 'distributed-repr.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20131017202428-04'00'\", 'page': 0, 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.08', 'source': 'distributed-repr.pdf', 'subject': 'gnuplot plot', 'title': 'countries.capitals.projections.eps', 'total_pages': 9, 'trapped': '', '_id': '96b771d8-052b-46f7-830f-77125a54be2a', '_collection_name': 'pdfs'}),\n",
       "  Document(page_content='trained on approximately one billion words from the news dataset.\\n6', metadata={'author': 'kaichen', 'creationDate': 'Wed May  1 15:07:42 2013', 'creator': 'gnuplot 4.4 patchlevel 3', 'file_path': 'distributed-repr.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20131017202428-04'00'\", 'page': 5, 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.08', 'source': 'distributed-repr.pdf', 'subject': 'gnuplot plot', 'title': 'countries.capitals.projections.eps', 'total_pages': 9, 'trapped': '', '_id': '3db359c1-88cc-427d-8d31-e4caa6d61dcd', '_collection_name': 'pdfs'})],\n",
       " 'answer': '[/INST] The self-attention mechanism in transformers allows each token in a sequence to attend to every other token and weigh their importance, helping the model understand relationships between tokens. This is different from traditional recurrent neural networks (RNNs) that only consider sequential information. By allowing tokens to attend to each other, self-attention enables the model to capture long-range dependencies and context. [/INST]'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
